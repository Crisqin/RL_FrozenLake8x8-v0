{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FrozenLake8x8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN6YbvtfT0yofBhb5/V841b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Crisqin/RL_FrozenLake8x8-v0/blob/main/FrozenLake8x8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpcvy3IQiOAc"
      },
      "source": [
        "# using genetic algorithm to solve open ai gym solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6_pFKRcazsJ",
        "outputId": "9f7b0ad3-0a69-4b45-ff63-b349842cd89f"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "env.seed(0)\n",
        "\n",
        "def evaluate_policy(env, policy):\n",
        "  total_rewards = 0.0\n",
        "  for _ in range(100):\n",
        "    obs = env.reset()\n",
        "    while True:\n",
        "      action = policy[obs]\n",
        "      obs, reward, done, info = env.step(action)\n",
        "      total_rewards += reward\n",
        "      if done:\n",
        "        break\n",
        "  return total_rewards/100\n",
        "\n",
        "def crossover(policy1, policy2):\n",
        "  new_policy = policy1.copy()\n",
        "  for i in range(16):\n",
        "      rand = np.random.uniform()\n",
        "      if rand > 0.5:\n",
        "          new_policy[i] = policy2[i]\n",
        "  return new_policy\n",
        "\n",
        "def mutation(policy):\n",
        "  new_policy = policy.copy()\n",
        "  for i in range(64):\n",
        "    rand = np.random.uniform()\n",
        "    if rand < 0.05:\n",
        "      new_policy[i] = np.random.choice(4)\n",
        "  return new_policy\n",
        "\n",
        "k=25\n",
        "policy_pop = [np.random.choice(4, size=((64))) for _ in range(100)]\n",
        "for idx in range(25):\n",
        "  policy_scores = [evaluate_policy(env, pp) for pp in policy_pop]\n",
        "  policy_ranks = list(reversed(np.argsort(policy_scores)))\n",
        "  elite_set= [policy_pop[x] for x in policy_ranks[:k]]\n",
        "  select_probs = np.array(policy_scores) / np.sum(policy_scores)\n",
        "  child_set = [crossover(\n",
        "      policy_pop[np.random.choice(range(100), p=select_probs)], \n",
        "      policy_pop[np.random.choice(range(100), p=select_probs)])\n",
        "      for _ in range(100 - k)]\n",
        "  k-=1\n",
        "  mutated_list = [mutation(c) for c in child_set]\n",
        "  policy_pop = elite_set\n",
        "  policy_pop += mutated_list\n",
        "policy_score = [evaluate_policy(env, pp) for pp in policy_pop]\n",
        "best_policy = policy_pop[np.argmax(policy_score)]\n",
        "print('Best actions score =', (np.max(policy_score)),'best actions =', best_policy.reshape(8,8))\n",
        "env.close()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best actions score = 0.93 best actions = [[2 2 3 2 1 1 2 2]\n",
            " [3 3 3 3 3 3 2 2]\n",
            " [2 3 3 0 2 1 2 2]\n",
            " [3 1 1 2 0 1 2 2]\n",
            " [2 0 3 1 1 1 3 2]\n",
            " [1 2 2 0 1 0 3 2]\n",
            " [3 2 0 0 3 3 0 2]\n",
            " [1 3 2 0 0 0 2 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vASgZezJiSZg"
      },
      "source": [
        "# policy_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "6fKo4WF4iYoU",
        "outputId": "92d62b4a-c184-49f8-c186-3e96d288fbf6"
      },
      "source": [
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def policy_evaluation(env, value_table, policy, gamma=0.9, threshold=1e-4):\n",
        "    delta = 2 * threshold\n",
        "    while delta > threshold:\n",
        "        # 1.存储旧的value表\n",
        "        new_value_table = np.zeros(env.nS)  # 此处不能用np.copy(value_table),因为更新V(s)用的是+=，所以若不置零则无限加和不收敛\n",
        "        for state in range(env.nS):\n",
        "            # 从当前state提取策略对应的action\n",
        "            action = policy[state]          # 可能会由于随机选择的动作不包含奖励=1的动作而得到无更新的new_value_table\n",
        "            # 2.更新V(s)\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                new_value_table[state] += prob * (reward + gamma*value_table[next_state])\n",
        "        delta = sum(np.fabs(new_value_table - value_table))\n",
        "        value_table = new_value_table\n",
        "    return value_table\n",
        "\n",
        "def policy_improvement(env, value_table, policy, gamma=0.9):\n",
        "    while True:\n",
        "        # 1.存储旧policy\n",
        "        old_policy = np.copy(policy)\n",
        "        for state in range(env.nS):\n",
        "            action_value = np.zeros(env.nA)\n",
        "            for action in range(env.nA):\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    action_value[action] += prob * (reward + gamma*value_table[next_state])\n",
        "            # 2.更新最优policy\n",
        "            policy[state] = np.argmax(action_value)\n",
        "        if np.all(policy == old_policy):  break\n",
        "    return policy\n",
        "    \n",
        "def policy_iteration(env, iterations, gamma=0.9):\n",
        "    env.reset()\n",
        "    start = time.time()\n",
        "    # 初始化策略-随机策略 (16个状态下的[0,4)策略)\n",
        "    policy = np.random.randint(low=0, high=env.nA, size=env.nS)\n",
        "    #policy = np.ones(env.observation_space.n)\n",
        "    # 初始化value表 (初始化0)\n",
        "    value_table = np.zeros(env.nS)\n",
        "    for step in range(iterations):\n",
        "        old_policy = np.copy(policy)\n",
        "        # 1.Policy Evaluation\n",
        "        value_table = policy_evaluation(env, value_table, policy, gamma)\n",
        "        # 2.Policy Improvement\n",
        "        policy = policy_improvement(env, value_table, policy, gamma)\n",
        "        # 3.判断终止条件\n",
        "        if np.all(policy == old_policy):\n",
        "            print('===== Policy Iteration ======\\nTime Consumption: {}s\\nIteration: {} steps\\nOptimal Policy(gamma={}): {}'.format(time.time()-start, step+1, gamma, policy))\n",
        "            break\n",
        "    return value_table, policy\n",
        "\n",
        "def play_game(env, policy, episodes=5, timesteps=150):\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        for t in range(timesteps):\n",
        "            action = policy[state]\n",
        "            state, reward, done, info = env.step(action)\n",
        "            if done:\n",
        "                print(\"===== Episode {} finished ====== \\n[Reward]: {} [Iteration]: {} steps\".format(episode+1, reward, t+1))\n",
        "                env.render()\n",
        "                break\n",
        "\n",
        "# 创建冰湖环境\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "# 策略迭代\n",
        "for i in np.arange(0,1,0.1):\n",
        "  value_table, policy = policy_iteration(env, iterations=100000, gamma=i)\n",
        "  # 使用迭代计算得到的策略打游戏\n",
        "  play_game(env, policy, episodes=3)\n",
        "env.close()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== Policy Iteration ======\n",
            "Time Consumption: 0.004427433013916016s\n",
            "Iteration: 2 steps\n",
            "Optimal Policy(gamma=0.0): [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "===== Policy Iteration ======\n",
            "Time Consumption: 0.049445390701293945s\n",
            "Iteration: 11 steps\n",
            "Optimal Policy(gamma=0.1): [1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 3. 2. 1. 1. 1. 1. 2. 0. 0. 2. 3. 2. 1.\n",
            " 3. 2. 3. 1. 0. 0. 2. 1. 3. 3. 0. 0. 2. 1. 3. 2. 0. 0. 0. 1. 3. 0. 0. 2.\n",
            " 0. 0. 1. 0. 0. 0. 0. 2. 1. 1. 0. 0. 1. 1. 1. 0.]\n",
            "===== Episode 1 finished ====== \n",
            "[Reward]: 1.0 [Iteration]: 125 steps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4ec501e8a5a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0mvalue_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0;31m# 使用迭代计算得到的策略打游戏\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-4ec501e8a5a1>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(env, policy, episodes, timesteps)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"===== Episode {} finished ====== \\n[Reward]: {} [Iteration]: {} steps\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/toy_text/frozen_lake.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastaction\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             outfile.write(\"  ({})\\n\".format(\n\u001b[0;32m--> 169\u001b[0;31m                 [\"Left\", \"Down\", \"Right\", \"Up\"][self.lastaction]))\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not numpy.float64"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx97w8BniY9X"
      },
      "source": [
        "# value_iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeAZ8YB0inCP",
        "outputId": "446f51f4-e3ae-4e3f-e6ed-f9ef4b68fd76"
      },
      "source": [
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def value_iteration(env, threshold=1e-4, gamma=0.9):\n",
        "    env.reset()\n",
        "    start = time.time()\n",
        "    # 初始化策略\n",
        "    policy = np.zeros(env.nS, dtype=int)   # 默认为float类型\n",
        "    # 初始化value表 (初始化0)\n",
        "    value_table = np.zeros(env.nS)\n",
        "    new_value_table = np.zeros(env.nS)\n",
        "    delta = 2 * threshold\n",
        "    while delta > threshold:\n",
        "        for state in range(env.nS):\n",
        "            action_value = np.zeros(env.nA)\n",
        "            for action in range(env.nA):\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    action_value[action] += prob * (reward + gamma*value_table[next_state])\n",
        "            # 1.利用max操作更新V(s)，区别与Policy Iteration\n",
        "            new_value_table[state] = max(action_value)\n",
        "            # 2.Policy Improvement\n",
        "            policy[state] = np.argmax(action_value)\n",
        "        delta = sum( np.fabs(new_value_table - value_table) )\n",
        "        value_table = np.copy(new_value_table)   # 注：需用copy拷贝副本，否则两个变量指向同一位置，则赋值时同时改变\n",
        "    print('===== Value Iteration ======\\nTime Consumption: {}s\\nIteration: {} steps\\nOptimal Policy(gamma={}): {}'.format(time.time()-start, 1, gamma, policy))\n",
        "    return value_table, policy\n",
        "\n",
        "def play_game(env, policy, episodes=5, timesteps=150):\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        for t in range(timesteps):\n",
        "            action = policy[state]\n",
        "            state, reward, done, info = env.step(action)\n",
        "            if done:\n",
        "                print(\"===== Episode {} finished ====== \\n[Reward]: {} [Iteration]: {} steps\".format(episode+1, reward, t+1))\n",
        "                env.render()\n",
        "                break\n",
        "\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "# 价值迭代\n",
        "for i in np.arange(0,1,0.1):\n",
        "  value_table, policy = value_iteration(env, gamma=i)\n",
        "  # 使用迭代计算得到的策略打游戏\n",
        "  play_game(env, policy, episodes=3)\n",
        "env.close()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== Value Iteration ======\n",
            "Time Consumption: 0.002713918685913086s\n",
            "Iteration: 1 steps\n",
            "Optimal Policy(gamma=0.0): [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            "===== Value Iteration ======\n",
            "Time Consumption: 0.006482839584350586s\n",
            "Iteration: 1 steps\n",
            "Optimal Policy(gamma=0.1): [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
            " 1 1 1 0 0 0 0 1 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 1 1 1 0]\n",
            "===== Value Iteration ======\n",
            "Time Consumption: 0.007903575897216797s\n",
            "Iteration: 1 steps\n",
            "Optimal Policy(gamma=0.2): [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1\n",
            " 1 3 2 0 0 0 1 1 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 1 1 1 0]\n",
            "===== Value Iteration ======\n",
            "Time Consumption: 0.010517120361328125s\n",
            "Iteration: 1 steps\n",
            "Optimal Policy(gamma=0.30000000000000004): [0 0 0 0 0 0 1 2 0 0 0 0 0 1 1 1 0 0 0 0 1 1 2 1 0 0 0 1 0 0 2 1 0 0 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 0 0 0 1 1 1 0]\n",
            "===== Value Iteration ======\n",
            "Time Consumption: 0.012689828872680664s\n",
            "Iteration: 1 steps\n",
            "Optimal Policy(gamma=0.4): [0 0 0 0 1 2 2 2 0 0 0 1 1 1 1 1 0 0 0 0 2 3 2 1 0 1 1 1 0 0 2 1 0 0 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0]\n",
            "===== Value Iteration ======\n",
            "Time Consumption: 0.01699090003967285s\n",
            "Iteration: 1 steps\n",
            "Optimal Policy(gamma=0.5): [0 1 2 2 2 2 2 2 1 1 2 3 2 2 1 1 1 2 0 0 2 3 2 1 3 2 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 1 1 0 0 1 1 1 0]\n",
            "===== Episode 1 finished ====== \n",
            "[Reward]: 1.0 [Iteration]: 81 steps\n",
            "  (Right)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF\u001b[41mG\u001b[0m\n",
            "===== Episode 2 finished ====== \n",
            "[Reward]: 0.0 [Iteration]: 47 steps\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFF\u001b[41mH\u001b[0mFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "===== Episode 3 finished ====== \n",
            "[Reward]: 1.0 [Iteration]: 47 steps\n",
            "  (Right)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF\u001b[41mG\u001b[0m\n",
            "===== Value Iteration ======\n",
            "Time Consumption: 0.04938697814941406s\n",
            "Iteration: 1 steps\n",
            "Optimal Policy(gamma=0.6000000000000001): [1 2 2 2 2 2 2 2 1 2 2 3 2 2 2 1 1 2 0 0 2 3 2 1 3 2 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 1 1 0 0 1 1 1 0]\n",
            "===== Episode 1 finished ====== \n",
            "[Reward]: 1.0 [Iteration]: 84 steps\n",
            "  (Right)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF\u001b[41mG\u001b[0m\n",
            "===== Episode 2 finished ====== \n",
            "[Reward]: 0.0 [Iteration]: 18 steps\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFF\u001b[41mH\u001b[0mFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "===== Episode 3 finished ====== \n",
            "[Reward]: 1.0 [Iteration]: 23 steps\n",
            "  (Right)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF\u001b[41mG\u001b[0m\n",
            "===== Value Iteration ======\n",
            "Time Consumption: 0.06681704521179199s\n",
            "Iteration: 1 steps\n",
            "Optimal Policy(gamma=0.7000000000000001): [3 2 2 2 2 2 2 2 3 2 2 3 2 2 2 1 2 2 0 0 2 3 2 1 3 2 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 1 1 0 0 1 1 1 0]\n",
            "===== Episode 1 finished ====== \n",
            "[Reward]: 1.0 [Iteration]: 41 steps\n",
            "  (Right)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF\u001b[41mG\u001b[0m\n",
            "===== Episode 2 finished ====== \n",
            "[Reward]: 1.0 [Iteration]: 86 steps\n",
            "  (Right)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF\u001b[41mG\u001b[0m\n",
            "===== Episode 3 finished ====== \n",
            "[Reward]: 0.0 [Iteration]: 11 steps\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFF\u001b[41mH\u001b[0mFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "===== Value Iteration ======\n",
            "Time Consumption: 0.09957146644592285s\n",
            "Iteration: 1 steps\n",
            "Optimal Policy(gamma=0.8): [3 2 2 2 2 2 2 2 3 2 2 3 2 2 2 1 3 2 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 1 1 0 0 1 1 1 0]\n",
            "===== Episode 2 finished ====== \n",
            "[Reward]: 0.0 [Iteration]: 17 steps\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FH\u001b[41mH\u001b[0mFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "===== Episode 3 finished ====== \n",
            "[Reward]: 1.0 [Iteration]: 41 steps\n",
            "  (Right)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF\u001b[41mG\u001b[0m\n",
            "===== Value Iteration ======\n",
            "Time Consumption: 0.11955785751342773s\n",
            "Iteration: 1 steps\n",
            "Optimal Policy(gamma=0.9): [3 2 2 2 2 2 2 2 3 3 3 3 2 2 2 1 3 3 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0]\n",
            "===== Episode 1 finished ====== \n",
            "[Reward]: 1.0 [Iteration]: 35 steps\n",
            "  (Right)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF\u001b[41mG\u001b[0m\n",
            "===== Episode 2 finished ====== \n",
            "[Reward]: 1.0 [Iteration]: 108 steps\n",
            "  (Right)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF\u001b[41mG\u001b[0m\n",
            "===== Episode 3 finished ====== \n",
            "[Reward]: 1.0 [Iteration]: 76 steps\n",
            "  (Right)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF\u001b[41mG\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}